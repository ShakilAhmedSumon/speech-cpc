{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CPC.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/ShakilAhmedSumon/speech-cpc/blob/main/CPC.ipynb",
      "authorship_tag": "ABX9TyM4QsZBOCVd/M70/9/YO8DS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShakilAhmedSumon/speech-cpc/blob/main/CPC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fIN_iaxM8Z5_"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import Sequence\n",
        "from random import shuffle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import os\n",
        "import logging\n",
        "import warnings\n",
        "from random import shuffle\n",
        "import numpy as np\n",
        "\n",
        "from keras.layers import Conv1D, BatchNormalization, LeakyReLU, Flatten, Dense, GRU, TimeDistributed, Input, Lambda\n",
        "from keras.layers import Dot, Lambda\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from keras import backend as K\n",
        "from keras.backend import expand_dims\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_logging(fname, level=logging.DEBUG):\n",
        "    \"\"\"\n",
        "    Create logger instance\n",
        "    :param fname: name of log file\n",
        "    :param level: log level\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    formatter = logging.Formatter('[%(levelname)s]%(asctime)s:%(name)s:%(message)s')\n",
        "    logger = logging.getLogger()\n",
        "    logger.setLevel(level)\n",
        "\n",
        "    # File Handler\n",
        "    fh = logging.FileHandler(fname)\n",
        "    fh.setLevel(level)\n",
        "    fh.setFormatter(formatter)\n",
        "    logger.addHandler(fh)\n",
        "\n",
        "    # Stream Handler\n",
        "    ch = logging.StreamHandler()\n",
        "    ch.setLevel(logging.WARNING)\n",
        "    ch.setFormatter(formatter)\n",
        "    logger.addHandler(ch)"
      ],
      "metadata": {
        "id": "5_e6gOkf-ZqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ContrastiveDataGenerator(Sequence):\n",
        "\n",
        "    def __init__(self, data_pth='../data', batch_size=10, shuffle=True, seed=42, categories=list(), normalize=True,\n",
        "                 fs=16000, chunk_size=4096, context_samples=5, contrastive_samples=1):\n",
        "        \"\"\"\n",
        "        Constructor\n",
        "\n",
        "        :param data_file: path to data file\n",
        "        :param meta_file:  path to meta file\n",
        "        :param batch_size: batch size\n",
        "        :param measurement_ids: list of measurement ids. Dedicated for CV\n",
        "        :param shuffle:\n",
        "        :param seed: random seed\n",
        "        :param test_mode: return samples and signal_ids\n",
        "        :param normalize: to normalize the data\n",
        "        \"\"\"\n",
        "        self.it = 0\n",
        "        self.shuffle = shuffle\n",
        "        self.data_pth = data_pth\n",
        "        self.normalize = normalize\n",
        "        self.fs = fs\n",
        "        self.batch_size = batch_size\n",
        "        self.seed = seed\n",
        "        self.context_samples = int(context_samples)\n",
        "        self.contrastive_samples = int(contrastive_samples)\n",
        "        self.chunk_size = int(chunk_size)\n",
        "\n",
        "        # Extract list of files from csv\n",
        "        # file_list = pd.read_csv(os.path.join(data_pth, 'train_curated.csv'))\n",
        "        file_list = [f for f in os.listdir('/content/drive/MyDrive/Librispeech/') if f.endswith('.flac')]\n",
        "        if len(categories) == 0:\n",
        "            # self.file_list = file_list.fname.tolist()\n",
        "            self.file_list = file_list\n",
        "        else:\n",
        "            self.file_list = file_list.query('labels in @categories').fname.tolist()\n",
        "        self.list_sz = len(self.file_list)\n",
        "        self.max_it = int(np.ceil(self.list_sz / self.batch_size))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.max_it\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"\n",
        "        Performs at the end of each epoch\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        l = self.file_list\n",
        "        shuffle(l)\n",
        "        self.file_list = l\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        \"\"\"\n",
        "        Return one batch\n",
        "        :param item:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        return self.__data_generation(item)\n",
        "\n",
        "    def __data_generation(self, it):\n",
        "        \"\"\"\n",
        "        Data generator\n",
        "        :param it:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        pos = np.minimum(it * self.batch_size, self.list_sz)\n",
        "        frames = (self.contrastive_samples+self.context_samples)*self.chunk_size\n",
        "\n",
        "        i = 0\n",
        "        context_batch = np.zeros([self.batch_size, self.context_samples, self.chunk_size])\n",
        "        contrastive_batch = np.zeros([self.batch_size, self.contrastive_samples, self.chunk_size])\n",
        "\n",
        "        while i < self.batch_size:\n",
        "            fname = self.file_list[pos]\n",
        "            pos = (pos+1) % self.list_sz\n",
        "            signal, sr = librosa.load('/content/drive/MyDrive/Librispeech/' + fname, sr=self.fs)\n",
        "            if signal.shape[0]-frames < 0:\n",
        "                logging.getLogger(__name__).info(' File {:s} is too short'.format(fname))\n",
        "            else:\n",
        "                random_shift = np.random.randint(signal.shape[0]-frames)\n",
        "                batch = signal[random_shift:(frames + random_shift)].reshape((-1, self.chunk_size), order='C')\n",
        "                context_batch[i, :, :] = batch[:self.context_samples, :]\n",
        "                contrastive_batch[i, :, :] = batch[self.context_samples:self.context_samples+self.contrastive_samples, :]\n",
        "                i +=1\n",
        "\n",
        "        # shuffle data\n",
        "        #idx = np.random.choice(range(self.batch_size), self.batch_size, replace=False)\n",
        "        #contrastive_batch = contrastive_batch[idx, :, :]\n",
        "        labels=np.zeros([self.batch_size, self.batch_size])\n",
        "        labels=np.identity(self.batch_size)\n",
        "        labels = labels[:, :, np.newaxis]\n",
        "        #labels[range(self.batch_size), idx] = 1\n",
        "        s = ([context_batch[:, :, :, np.newaxis], contrastive_batch[:, :, :, np.newaxis]], labels)\n",
        "        return s"
      ],
      "metadata": {
        "id": "98VH5wh__I2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MpgdzBdTKpzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDmOsZ4p_SoZ",
        "outputId": "7d0d3bd5-375e-40ff-9490-4df73e7a52dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# os.listdir('/content/drive/MyDrive/Librispeech/')"
      ],
      "metadata": {
        "id": "JeByafK7EBo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [f for f in os.listdir('/content/drive/MyDrive/Librispeech/') if f.endswith('.flac')]"
      ],
      "metadata": {
        "id": "BPQBCWA70Ha0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !ls"
      ],
      "metadata": {
        "id": "N4GIYrGWEfxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_list = os.listdir()"
      ],
      "metadata": {
        "id": "I8x0wLe1EndI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_encoder(x, emb_size):\n",
        "    \"\"\"\n",
        "    Create encoder\n",
        "    :param x:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    with tf.name_scope('Encoder'):\n",
        "        with tf.name_scope('embedding_level_1'):\n",
        "            x = Conv1D(filters=10, strides=5, kernel_size=3)(x)\n",
        "            x = LeakyReLU()(x)\n",
        "            x = BatchNormalization()(x)\n",
        "\n",
        "        with tf.name_scope('embedding_level_2'):\n",
        "            x = Conv1D(filters=8, strides=4, kernel_size=3)(x)\n",
        "            x = LeakyReLU()(x)\n",
        "            x = BatchNormalization()(x)\n",
        "\n",
        "        with tf.name_scope('embedding_level_2'):\n",
        "            x = Conv1D(filters=4, strides=2, kernel_size=3)(x)\n",
        "            x = LeakyReLU()(x)\n",
        "            x = BatchNormalization()(x)\n",
        "\n",
        "        with tf.name_scope('embedding_level_4'):\n",
        "            x = Conv1D(filters=4, strides=2, kernel_size=3)(x)\n",
        "            x = LeakyReLU()(x)\n",
        "            x = BatchNormalization()(x)\n",
        "\n",
        "        with tf.name_scope('embedding_level_5'):\n",
        "            x = Conv1D(filters=4, strides=2, kernel_size=3)(x)\n",
        "            x = LeakyReLU()(x)\n",
        "            x = BatchNormalization()(x)\n",
        "\n",
        "        with tf.name_scope('embedding_dense'):\n",
        "            x = Flatten()(x)\n",
        "            x = Dense(units=emb_size, activation='relu')(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "eN4C2AFQEvTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def network_autoregressive(x, code_size):\n",
        "    \"\"\"\n",
        "    Define the network that integrates information along the sequence\n",
        "    :param x:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return GRU(units=code_size, return_sequences=False, name='autoregressive_context')(x)\n"
      ],
      "metadata": {
        "id": "PBLKRaiDGkle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Contrastive loss function (eq. 4 from the original article)\n",
        "    # https://datascience.stackexchange.com/questions/25029/custom-loss-function-with-additional-parameter-in-keras\n",
        "    :param y_true: labels (0, 1), where 0 means the sample was drawn from noisy distribution; 1 means the sample was\n",
        "    drawn from the target distribution.\n",
        "    :param y_pred: density ratio (f value from the original article)\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    with tf.name_scope('custom_loss_function'):\n",
        "        divident = K.sum(K.dot(y_true, y_pred), axis=1)\n",
        "        divider = K.sum(y_pred, axis=1) + K.epsilon()\n",
        "        l = -K.log(divident / divider)\n",
        "    return l*1e4"
      ],
      "metadata": {
        "id": "ubB_bldXGnlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(chunk_size, context_samples=100, contrastive_samples=10, emd_size=512, gru_size=256):\n",
        "    \"\"\"\n",
        "\n",
        "    :param chunk_size:\n",
        "    :param context_samples:\n",
        "    :param contrastive_samples:\n",
        "    :param emd_size:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    K.set_learning_phase(1)\n",
        "\n",
        "    # Define encoder model\n",
        "    encoder_input = Input(shape=[chunk_size, 1])\n",
        "    encoder_model = Model(encoder_input, get_encoder(encoder_input, emb_size=emd_size), name='encoder')\n",
        "    encoder_model.summary()\n",
        "\n",
        "    # Define rest of the model\n",
        "    x_input = Input(shape=[context_samples, chunk_size, 1], name='context_data')\n",
        "    y_input = Input(shape=[contrastive_samples, chunk_size, 1], name='contrastive_data')\n",
        "\n",
        "    # Workaround context\n",
        "    x_encoded = TimeDistributed(encoder_model, name='Historical_embeddings')(x_input)\n",
        "    context = network_autoregressive(x_encoded, gru_size)\n",
        "    context = Lambda(lambda x: expand_dims(x, axis=-1), name='transpose_context')(context)\n",
        "\n",
        "    # Make predictions for the next predict_terms timesteps\n",
        "    z = TimeDistributed(encoder_model, name='Contrastive_embeddings')(y_input)\n",
        "    # Equation 3\n",
        "    z2 = Dense(units=gru_size, name='W', use_bias=False)(z)\n",
        "    z2 = Lambda(lambda x: K.permute_dimensions(x, (0, 2, 1)), name='transpose')(z2)\n",
        "    d = Lambda(lambda x: Dot(axes=1)(x), name='multiplication')([z2, context])\n",
        "\n",
        "    f = Lambda(lambda x: K.exp(x), name='exponent')(d)\n",
        "\n",
        "    # Model\n",
        "    cpc_model = Model(inputs=[x_input, y_input], outputs=f) #, y_labels\n",
        "    cpc_model.summary()\n",
        "    return cpc_model"
      ],
      "metadata": {
        "id": "FHefrS_RGuQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    tmr = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
        "    # params\n",
        "    K.set_learning_phase(1)\n",
        "    chunk_size = 4096\n",
        "    context_samples = 5\n",
        "    contrastive_samples = 1\n",
        "    emd_size = 512\n",
        "    batch_size = 8\n",
        "\n",
        "    params = {'model_name': 'cpc1'}\n",
        "    params.update({'checkpointer': {'verbose': 1,\n",
        "                                   'save_best_only': True,\n",
        "                                   'mode': 'min',\n",
        "                                    'monitor': 'loss'}})\n",
        "\n",
        "    model_params = {'chunk_size': chunk_size,\n",
        "                    'context_samples': context_samples,\n",
        "                    'contrastive_samples': contrastive_samples,\n",
        "                    'emd_size': emd_size}\n",
        "\n",
        "    #categories = ['Marimba_and_xylophone', 'Scissors', 'Gong', 'Printer', 'Keys_jangling', 'Zipper_(clothing)',\n",
        "    #              'Computer_keyboard', 'Finger_snapping']\n",
        "\n",
        "    categories = ()\n",
        "    gen_params = {'categories': categories,\n",
        "                  'data_pth': '/',\n",
        "                  'batch_size': batch_size,\n",
        "                  'shuffle': True,\n",
        "                  'seed': 42,\n",
        "                  'chunk_size': chunk_size,\n",
        "                  'context_samples': context_samples,\n",
        "                  'contrastive_samples': contrastive_samples}\n",
        "\n",
        "    output_folder = 'models'\n",
        "    tensorboard = TensorBoard(log_dir='./logs/' + 'cpc' + '_' + tmr,\n",
        "                              write_graph=True)\n",
        "    checkpointer = ModelCheckpoint(filepath=os.path.join(output_folder, params.get('model_name')+'.hdf5'),\n",
        "                                   **params['checkpointer'])\n",
        "\n",
        "    callbacks = [tensorboard, checkpointer]\n",
        "    model = get_model(**model_params)\n",
        "\n",
        "    data_gen = ContrastiveDataGenerator(**gen_params)\n",
        "\n",
        "    model.compile(loss=loss_fn, optimizer=Adam(lr=1e-5))\n",
        "    model.fit_generator(generator=data_gen, epochs=10, callbacks=callbacks)"
      ],
      "metadata": {
        "id": "uOnunLBYGzMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kh5dIZlKG3Z_",
        "outputId": "1fa8713d-2ee3-4c8e-881e-1c50eb209fd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:414: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 4096, 1)]         0         \n",
            "                                                                 \n",
            " conv1d_5 (Conv1D)           (None, 819, 10)           40        \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 819, 10)           0         \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 819, 10)          40        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv1d_6 (Conv1D)           (None, 205, 8)            248       \n",
            "                                                                 \n",
            " leaky_re_lu_6 (LeakyReLU)   (None, 205, 8)            0         \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 205, 8)           32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv1d_7 (Conv1D)           (None, 102, 4)            100       \n",
            "                                                                 \n",
            " leaky_re_lu_7 (LeakyReLU)   (None, 102, 4)            0         \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 102, 4)           16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv1d_8 (Conv1D)           (None, 50, 4)             52        \n",
            "                                                                 \n",
            " leaky_re_lu_8 (LeakyReLU)   (None, 50, 4)             0         \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 50, 4)            16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv1d_9 (Conv1D)           (None, 24, 4)             52        \n",
            "                                                                 \n",
            " leaky_re_lu_9 (LeakyReLU)   (None, 24, 4)             0         \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 24, 4)            16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 96)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 512)               49664     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 50,276\n",
            "Trainable params: 50,216\n",
            "Non-trainable params: 60\n",
            "_________________________________________________________________\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " contrastive_data (InputLayer)  [(None, 1, 4096, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " context_data (InputLayer)      [(None, 5, 4096, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " Contrastive_embeddings (TimeDi  (None, 1, 512)      50276       ['contrastive_data[0][0]']       \n",
            " stributed)                                                                                       \n",
            "                                                                                                  \n",
            " Historical_embeddings (TimeDis  (None, 5, 512)      50276       ['context_data[0][0]']           \n",
            " tributed)                                                                                        \n",
            "                                                                                                  \n",
            " W (Dense)                      (None, 1, 256)       131072      ['Contrastive_embeddings[0][0]'] \n",
            "                                                                                                  \n",
            " autoregressive_context (GRU)   (None, 256)          591360      ['Historical_embeddings[0][0]']  \n",
            "                                                                                                  \n",
            " transpose (Lambda)             (None, 256, 1)       0           ['W[0][0]']                      \n",
            "                                                                                                  \n",
            " transpose_context (Lambda)     (None, 256, 1)       0           ['autoregressive_context[0][0]'] \n",
            "                                                                                                  \n",
            " multiplication (Lambda)        (None, 1, 1)         0           ['transpose[0][0]',              \n",
            "                                                                  'transpose_context[0][0]']      \n",
            "                                                                                                  \n",
            " exponent (Lambda)              (None, 1, 1)         0           ['multiplication[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 772,708\n",
            "Trainable params: 772,648\n",
            "Non-trainable params: 60\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0013\n",
            "Epoch 00001: loss improved from inf to 0.00127, saving model to models/cpc1.hdf5\n",
            "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
            "7/7 [==============================] - 15s 1s/step - loss: 0.0013\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - ETA: 0s - loss: 8.8343e-04\n",
            "Epoch 00002: loss improved from 0.00127 to 0.00088, saving model to models/cpc1.hdf5\n",
            "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
            "7/7 [==============================] - 1s 121ms/step - loss: 8.8343e-04\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - ETA: 0s - loss: 6.3862e-04\n",
            "Epoch 00003: loss improved from 0.00088 to 0.00064, saving model to models/cpc1.hdf5\n",
            "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
            "7/7 [==============================] - 1s 126ms/step - loss: 6.3862e-04\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - ETA: 0s - loss: 5.4283e-04\n",
            "Epoch 00004: loss improved from 0.00064 to 0.00054, saving model to models/cpc1.hdf5\n",
            "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
            "7/7 [==============================] - 1s 117ms/step - loss: 5.4283e-04\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - ETA: 0s - loss: 4.4703e-04\n",
            "Epoch 00005: loss improved from 0.00054 to 0.00045, saving model to models/cpc1.hdf5\n",
            "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
            "7/7 [==============================] - 1s 110ms/step - loss: 4.4703e-04\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - ETA: 0s - loss: 4.3639e-04\n",
            "Epoch 00006: loss improved from 0.00045 to 0.00044, saving model to models/cpc1.hdf5\n",
            "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
            "7/7 [==============================] - 1s 125ms/step - loss: 4.3639e-04\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - ETA: 0s - loss: 2.8738e-04\n",
            "Epoch 00007: loss improved from 0.00044 to 0.00029, saving model to models/cpc1.hdf5\n",
            "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
            "7/7 [==============================] - 1s 123ms/step - loss: 2.8738e-04\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - ETA: 0s - loss: 2.3416e-04\n",
            "Epoch 00008: loss improved from 0.00029 to 0.00023, saving model to models/cpc1.hdf5\n",
            "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
            "7/7 [==============================] - 1s 107ms/step - loss: 2.3416e-04\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - ETA: 0s - loss: 2.3416e-04\n",
            "Epoch 00009: loss did not improve from 0.00023\n",
            "7/7 [==============================] - 1s 90ms/step - loss: 2.3416e-04\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - ETA: 0s - loss: 3.1931e-04\n",
            "Epoch 00010: loss did not improve from 0.00023\n",
            "7/7 [==============================] - 1s 91ms/step - loss: 3.1931e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio_array = []\n",
        "audio_files = [f for f in os.listdir('sample_data/') if f.endswith('.wav')]"
      ],
      "metadata": {
        "id": "aMVdjvxWHhGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "signal, sr = librosa.load('sample_data/' + audio_files[0], 20480)"
      ],
      "metadata": {
        "id": "xEGVyyjfL4xO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4azp78SBMGiR",
        "outputId": "4fda4879-fadc-4995-cedf-fd413a84171e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0a2b400e_nohash_4.wav',\n",
              " '0a2b400e_nohash_2.wav',\n",
              " '0a2b400e_nohash_3.wav',\n",
              " '0a2b400e_nohash_1.wav',\n",
              " '0a2b400e_nohash_0.wav']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "signal = signal.reshape(-1, 4096)"
      ],
      "metadata": {
        "id": "7WIrT2auMTxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "signal.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEmmhGP-MtNK",
        "outputId": "81d08b4e-1894-4c72-a466-84270e73e484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 4096)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = Input(shape=[4096, 1])\n",
        "encoder_model = Model(encoder_input, get_encoder(encoder_input, emb_size=512), name='encoder')\n",
        "encoder_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOsG51m-M1uD",
        "outputId": "74599d42-8def-4389-a24c-ce488c947ade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 4096, 1)]         0         \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 819, 10)           40        \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 819, 10)           0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 819, 10)          40        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 205, 8)            248       \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 205, 8)            0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 205, 8)           32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 102, 4)            100       \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 102, 4)            0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 102, 4)           16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 50, 4)             52        \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 50, 4)             0         \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 50, 4)            16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv1d_4 (Conv1D)           (None, 24, 4)             52        \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 24, 4)             0         \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 24, 4)            16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 96)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               49664     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 50,276\n",
            "Trainable params: 50,216\n",
            "Non-trainable params: 60\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_model(signal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOQ1l8p5NYfC",
        "outputId": "4bcf42b6-c6cd-4526-affb-58347c7fcd6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 512), dtype=float32, numpy=\n",
              "array([[0.00046377, 0.        , 0.        , ..., 0.0025437 , 0.00441912,\n",
              "        0.00220322],\n",
              "       [0.        , 0.00672512, 0.00314798, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.00911165, 0.        , 0.        , ..., 0.        , 0.00637178,\n",
              "        0.        ],\n",
              "       [0.01837928, 0.00439205, 0.00848518, ..., 0.        , 0.00391455,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.0027806 , ..., 0.00072552, 0.        ,\n",
              "        0.        ]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cpc_model  = tf.keras.models.load_model('models/cpc1.hdf5',custom_objects={'loss_fn': loss_fn})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCP2UT3hNsqf",
        "outputId": "3ef00854-e19f-4aa1-b991-054b84116095"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs = Input(None, 5,4096,1)"
      ],
      "metadata": {
        "id": "fU7LdAUCSE_A"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder = Model(input=inputs, output=model.get_layer('Historical_embeddings'))"
      ],
      "metadata": {
        "id": "5HT-wse7RW8B"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QawmhePOR0Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Designing model with grouped convolution for minimization of the number of parameters"
      ],
      "metadata": {
        "id": "hJ-BEmZeutc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_model = Input(shape=[5, 512])\n",
        "\n",
        "x = Conv1D(filters=8, groups=4, strides=2, kernel_size=8, padding = 'same')(input_model)\n",
        "x = LeakyReLU()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv1D(filters=4, groups = 4, kernel_size=3)(x)\n",
        "x = LeakyReLU()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(units=10, activation='softmax')(x)\n",
        "\n",
        "\n",
        "grouped_model = Model(input_model, x)\n",
        "\n",
        "grouped_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nerdH9YWu9xi",
        "outputId": "8e53e99b-75d4-432f-8367-de6246a9e107"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_17 (InputLayer)       [(None, 5, 512)]          0         \n",
            "                                                                 \n",
            " conv1d_22 (Conv1D)          (None, 3, 8)              8200      \n",
            "                                                                 \n",
            " leaky_re_lu_12 (LeakyReLU)  (None, 3, 8)              0         \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 3, 8)             32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv1d_23 (Conv1D)          (None, 1, 4)              28        \n",
            "                                                                 \n",
            " leaky_re_lu_13 (LeakyReLU)  (None, 1, 4)              0         \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 1, 4)             16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 10)                50        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,326\n",
            "Trainable params: 8,302\n",
            "Non-trainable params: 24\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Bb1rJ_8SH7_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depth-wise separable convolution"
      ],
      "metadata": {
        "id": "qp-zFaDHKhlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_model = Input(shape=[5, 512])\n",
        "\n",
        "d = tf.keras.layers.DepthwiseConv1D(strides=2, kernel_size=3, depth_multiplier=2)(input_model)\n",
        "x = LeakyReLU()(d)\n",
        "x = BatchNormalization()(x)\n",
        "x = tf.keras.layers.DepthwiseConv1D(strides=2, kernel_size=2, depth_multiplier=1)(input_model)\n",
        "x = LeakyReLU()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(units=10, activation='softmax')(x)\n",
        "\n",
        "\n",
        "depthwise_model = Model(input_model, x)\n",
        "\n",
        "depthwise_model.summary()"
      ],
      "metadata": {
        "id": "zJluHNIAKoqq",
        "outputId": "985e49b2-f3c2-4e74-f3a3-45666c775b6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_26 (InputLayer)       [(None, 5, 512)]          0         \n",
            "                                                                 \n",
            " depthwise_conv1d_8 (Depthwi  (None, 2, 512)           1536      \n",
            " seConv1D)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_20 (LeakyReLU)  (None, 2, 512)            0         \n",
            "                                                                 \n",
            " batch_normalization_20 (Bat  (None, 2, 512)           2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 10)                10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,834\n",
            "Trainable params: 12,810\n",
            "Non-trainable params: 1,024\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Z74xpaMcL5-7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}