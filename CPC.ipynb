{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CPC.ipynb",
      "provenance": [],
      "mount_file_id": "1NCCekMjkVai1aJuqzcktdtbIkPjgbKVm",
      "authorship_tag": "ABX9TyN6BBgD502QkrUj8FrkID2o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShakilAhmedSumon/speech-cpc/blob/main/CPC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fIN_iaxM8Z5_"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import Sequence\n",
        "from random import shuffle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import os\n",
        "import logging\n",
        "import warnings\n",
        "from random import shuffle\n",
        "import numpy as np\n",
        "\n",
        "from keras.layers import Conv1D, BatchNormalization, LeakyReLU, Flatten, Dense, GRU, TimeDistributed, Input, Lambda\n",
        "from keras.layers import Dot, Lambda\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from keras import backend as K\n",
        "from keras.backend import expand_dims\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_logging(fname, level=logging.DEBUG):\n",
        "    \"\"\"\n",
        "    Create logger instance\n",
        "    :param fname: name of log file\n",
        "    :param level: log level\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    formatter = logging.Formatter('[%(levelname)s]%(asctime)s:%(name)s:%(message)s')\n",
        "    logger = logging.getLogger()\n",
        "    logger.setLevel(level)\n",
        "\n",
        "    # File Handler\n",
        "    fh = logging.FileHandler(fname)\n",
        "    fh.setLevel(level)\n",
        "    fh.setFormatter(formatter)\n",
        "    logger.addHandler(fh)\n",
        "\n",
        "    # Stream Handler\n",
        "    ch = logging.StreamHandler()\n",
        "    ch.setLevel(logging.WARNING)\n",
        "    ch.setFormatter(formatter)\n",
        "    logger.addHandler(ch)"
      ],
      "metadata": {
        "id": "5_e6gOkf-ZqF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ContrastiveDataGenerator(Sequence):\n",
        "\n",
        "    def __init__(self, data_pth='../data', batch_size=10, shuffle=True, seed=42, categories=list(), normalize=True,\n",
        "                 fs=16000, chunk_size=4096, context_samples=5, contrastive_samples=1):\n",
        "        \"\"\"\n",
        "        Constructor\n",
        "\n",
        "        :param data_file: path to data file\n",
        "        :param meta_file:  path to meta file\n",
        "        :param batch_size: batch size\n",
        "        :param measurement_ids: list of measurement ids. Dedicated for CV\n",
        "        :param shuffle:\n",
        "        :param seed: random seed\n",
        "        :param test_mode: return samples and signal_ids\n",
        "        :param normalize: to normalize the data\n",
        "        \"\"\"\n",
        "        self.it = 0\n",
        "        self.shuffle = shuffle\n",
        "        self.data_pth = data_pth\n",
        "        self.normalize = normalize\n",
        "        self.fs = fs\n",
        "        self.batch_size = batch_size\n",
        "        self.seed = seed\n",
        "        self.context_samples = int(context_samples)\n",
        "        self.contrastive_samples = int(contrastive_samples)\n",
        "        self.chunk_size = int(chunk_size)\n",
        "\n",
        "        # Extract list of files from csv\n",
        "        # file_list = pd.read_csv(os.path.join(data_pth, 'train_curated.csv'))\n",
        "        file_list = [f for f in os.listdir('/content/drive/MyDrive/Librispeech/') if f.endswith('.flac')]\n",
        "        if len(categories) == 0:\n",
        "            # self.file_list = file_list.fname.tolist()\n",
        "            self.file_list = file_list\n",
        "        else:\n",
        "            self.file_list = file_list.query('labels in @categories').fname.tolist()\n",
        "        self.list_sz = len(self.file_list)\n",
        "        self.max_it = int(np.ceil(self.list_sz / self.batch_size))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.max_it\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"\n",
        "        Performs at the end of each epoch\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        l = self.file_list\n",
        "        shuffle(l)\n",
        "        self.file_list = l\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        \"\"\"\n",
        "        Return one batch\n",
        "        :param item:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        return self.__data_generation(item)\n",
        "\n",
        "    def __data_generation(self, it):\n",
        "        \"\"\"\n",
        "        Data generator\n",
        "        :param it:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        pos = np.minimum(it * self.batch_size, self.list_sz)\n",
        "        frames = (self.contrastive_samples+self.context_samples)*self.chunk_size\n",
        "\n",
        "        i = 0\n",
        "        context_batch = np.zeros([self.batch_size, self.context_samples, self.chunk_size])\n",
        "        contrastive_batch = np.zeros([self.batch_size, self.contrastive_samples, self.chunk_size])\n",
        "\n",
        "        while i < self.batch_size:\n",
        "            fname = self.file_list[pos]\n",
        "            pos = (pos+1) % self.list_sz\n",
        "            signal, sr = librosa.load('/content/drive/MyDrive/Librispeech/' + fname, sr=self.fs)\n",
        "            if signal.shape[0]-frames < 0:\n",
        "                logging.getLogger(__name__).info(' File {:s} is too short'.format(fname))\n",
        "            else:\n",
        "                random_shift = np.random.randint(signal.shape[0]-frames)\n",
        "                batch = signal[random_shift:(frames + random_shift)].reshape((-1, self.chunk_size), order='C')\n",
        "                context_batch[i, :, :] = batch[:self.context_samples, :]\n",
        "                contrastive_batch[i, :, :] = batch[self.context_samples:self.context_samples+self.contrastive_samples, :]\n",
        "                i +=1\n",
        "\n",
        "        # shuffle data\n",
        "        #idx = np.random.choice(range(self.batch_size), self.batch_size, replace=False)\n",
        "        #contrastive_batch = contrastive_batch[idx, :, :]\n",
        "        labels=np.zeros([self.batch_size, self.batch_size])\n",
        "        labels=np.identity(self.batch_size)\n",
        "        labels = labels[:, :, np.newaxis]\n",
        "        #labels[range(self.batch_size), idx] = 1\n",
        "        s = ([context_batch[:, :, :, np.newaxis], contrastive_batch[:, :, :, np.newaxis]], labels)\n",
        "        return s"
      ],
      "metadata": {
        "id": "98VH5wh__I2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDmOsZ4p_SoZ",
        "outputId": "7d0d3bd5-375e-40ff-9490-4df73e7a52dd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('/content/drive/MyDrive/Librispeech/')"
      ],
      "metadata": {
        "id": "JeByafK7EBo1",
        "outputId": "372955f5-ab8f-45ac-a035-5ec0eee42b75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['84-121123-0000.flac',\n",
              " '84-121123-0001.flac',\n",
              " '84-121123-0007.flac',\n",
              " '174-50561-0002.flac',\n",
              " '174-50561-0004.flac',\n",
              " '84-121123-0013.flac',\n",
              " '174-50561-0009.flac',\n",
              " '84-121123-0012.flac',\n",
              " '174-50561-0012.flac',\n",
              " '174-50561-0014.flac',\n",
              " '174-50561-0018.flac',\n",
              " '84-121123-0019.flac',\n",
              " '84-121123-0022.flac',\n",
              " '84-121123-0009.flac',\n",
              " '84-121123-0011.flac',\n",
              " '84-121123-0014.flac',\n",
              " '84-121123-0004.flac',\n",
              " '84-121123-0015.flac',\n",
              " '174-50561-0017.flac',\n",
              " '84-121123-0027.flac',\n",
              " '174-50561-0003.flac',\n",
              " '174-50561-0000.flac',\n",
              " '84-121123-0018.flac',\n",
              " '174-50561-0019.flac',\n",
              " '84-121123-0006.flac',\n",
              " '84-121123-0021.flac',\n",
              " '84-121123-0003.flac',\n",
              " '84-121123-0023.flac',\n",
              " '174-50561-0006.flac',\n",
              " '174-50561-0005.flac',\n",
              " '84-121123-0028.flac',\n",
              " '84-121123-0010.flac',\n",
              " '84-121123-0025.flac',\n",
              " '84-121123-0008.flac',\n",
              " '84-121123-0020.flac',\n",
              " '84-121123-0016.flac',\n",
              " '174-50561-0007.flac',\n",
              " '174-50561-0013.flac',\n",
              " '174-50561-0011.flac',\n",
              " '84-121123-0017.flac',\n",
              " '84-121123-0024.flac',\n",
              " '174-50561-0016.flac',\n",
              " '84-121123-0002.flac',\n",
              " '174-50561-0010.flac',\n",
              " '174-50561-0008.flac',\n",
              " '174-50561-0015.flac',\n",
              " '84-121123-0026.flac',\n",
              " '174-50561-0001.flac',\n",
              " '84-121123-0005.flac',\n",
              " '.ipynb_checkpoints']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[f for f in os.listdir('/content/drive/MyDrive/Librispeech/') if f.endswith('.flac')]"
      ],
      "metadata": {
        "id": "BPQBCWA70Ha0",
        "outputId": "87e42020-e041-4484-8d48-ec116d46a5a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['84-121123-0000.flac',\n",
              " '84-121123-0001.flac',\n",
              " '84-121123-0007.flac',\n",
              " '174-50561-0002.flac',\n",
              " '174-50561-0004.flac',\n",
              " '84-121123-0013.flac',\n",
              " '174-50561-0009.flac',\n",
              " '84-121123-0012.flac',\n",
              " '174-50561-0012.flac',\n",
              " '174-50561-0014.flac',\n",
              " '174-50561-0018.flac',\n",
              " '84-121123-0019.flac',\n",
              " '84-121123-0022.flac',\n",
              " '84-121123-0009.flac',\n",
              " '84-121123-0011.flac',\n",
              " '84-121123-0014.flac',\n",
              " '84-121123-0004.flac',\n",
              " '84-121123-0015.flac',\n",
              " '174-50561-0017.flac',\n",
              " '84-121123-0027.flac',\n",
              " '174-50561-0003.flac',\n",
              " '174-50561-0000.flac',\n",
              " '84-121123-0018.flac',\n",
              " '174-50561-0019.flac',\n",
              " '84-121123-0006.flac',\n",
              " '84-121123-0021.flac',\n",
              " '84-121123-0003.flac',\n",
              " '84-121123-0023.flac',\n",
              " '174-50561-0006.flac',\n",
              " '174-50561-0005.flac',\n",
              " '84-121123-0028.flac',\n",
              " '84-121123-0010.flac',\n",
              " '84-121123-0025.flac',\n",
              " '84-121123-0008.flac',\n",
              " '84-121123-0020.flac',\n",
              " '84-121123-0016.flac',\n",
              " '174-50561-0007.flac',\n",
              " '174-50561-0013.flac',\n",
              " '174-50561-0011.flac',\n",
              " '84-121123-0017.flac',\n",
              " '84-121123-0024.flac',\n",
              " '174-50561-0016.flac',\n",
              " '84-121123-0002.flac',\n",
              " '174-50561-0010.flac',\n",
              " '174-50561-0008.flac',\n",
              " '174-50561-0015.flac',\n",
              " '84-121123-0026.flac',\n",
              " '174-50561-0001.flac',\n",
              " '84-121123-0005.flac']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4GIYrGWEfxc",
        "outputId": "1d650db5-4dc0-426f-e609-c7e3b4bb9204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "174-50561-0000.flac  174-50561-0017.flac  84-121123-0014.flac\n",
            "174-50561-0001.flac  174-50561-0018.flac  84-121123-0015.flac\n",
            "174-50561-0002.flac  174-50561-0019.flac  84-121123-0016.flac\n",
            "174-50561-0003.flac  84-121123-0000.flac  84-121123-0017.flac\n",
            "174-50561-0004.flac  84-121123-0001.flac  84-121123-0018.flac\n",
            "174-50561-0005.flac  84-121123-0002.flac  84-121123-0019.flac\n",
            "174-50561-0006.flac  84-121123-0003.flac  84-121123-0020.flac\n",
            "174-50561-0007.flac  84-121123-0004.flac  84-121123-0021.flac\n",
            "174-50561-0008.flac  84-121123-0005.flac  84-121123-0022.flac\n",
            "174-50561-0009.flac  84-121123-0006.flac  84-121123-0023.flac\n",
            "174-50561-0010.flac  84-121123-0007.flac  84-121123-0024.flac\n",
            "174-50561-0011.flac  84-121123-0008.flac  84-121123-0025.flac\n",
            "174-50561-0012.flac  84-121123-0009.flac  84-121123-0026.flac\n",
            "174-50561-0013.flac  84-121123-0010.flac  84-121123-0027.flac\n",
            "174-50561-0014.flac  84-121123-0011.flac  84-121123-0028.flac\n",
            "174-50561-0015.flac  84-121123-0012.flac\n",
            "174-50561-0016.flac  84-121123-0013.flac\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_list = os.listdir()"
      ],
      "metadata": {
        "id": "I8x0wLe1EndI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_encoder(x, emb_size):\n",
        "    \"\"\"\n",
        "    Create encoder\n",
        "    :param x:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    with tf.name_scope('Encoder'):\n",
        "        with tf.name_scope('embedding_level_1'):\n",
        "            x = Conv1D(filters=10, strides=5, kernel_size=3)(x)\n",
        "            x = LeakyReLU()(x)\n",
        "            x = BatchNormalization()(x)\n",
        "\n",
        "        with tf.name_scope('embedding_level_2'):\n",
        "            x = Conv1D(filters=8, strides=4, kernel_size=3)(x)\n",
        "            x = LeakyReLU()(x)\n",
        "            x = BatchNormalization()(x)\n",
        "\n",
        "        with tf.name_scope('embedding_level_2'):\n",
        "            x = Conv1D(filters=4, strides=2, kernel_size=3)(x)\n",
        "            x = LeakyReLU()(x)\n",
        "            x = BatchNormalization()(x)\n",
        "\n",
        "        with tf.name_scope('embedding_level_4'):\n",
        "            x = Conv1D(filters=4, strides=2, kernel_size=3)(x)\n",
        "            x = LeakyReLU()(x)\n",
        "            x = BatchNormalization()(x)\n",
        "\n",
        "        with tf.name_scope('embedding_level_5'):\n",
        "            x = Conv1D(filters=4, strides=2, kernel_size=3)(x)\n",
        "            x = LeakyReLU()(x)\n",
        "            x = BatchNormalization()(x)\n",
        "\n",
        "        with tf.name_scope('embedding_dense'):\n",
        "            x = Flatten()(x)\n",
        "            x = Dense(units=emb_size, activation='relu')(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "eN4C2AFQEvTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def network_autoregressive(x, code_size):\n",
        "    \"\"\"\n",
        "    Define the network that integrates information along the sequence\n",
        "    :param x:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return GRU(units=code_size, return_sequences=False, name='autoregressive_context')(x)\n"
      ],
      "metadata": {
        "id": "PBLKRaiDGkle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Contrastive loss function (eq. 4 from the original article)\n",
        "    # https://datascience.stackexchange.com/questions/25029/custom-loss-function-with-additional-parameter-in-keras\n",
        "    :param y_true: labels (0, 1), where 0 means the sample was drawn from noisy distribution; 1 means the sample was\n",
        "    drawn from the target distribution.\n",
        "    :param y_pred: density ratio (f value from the original article)\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    with tf.name_scope('custom_loss_function'):\n",
        "        divident = K.sum(K.dot(y_true, y_pred), axis=1)\n",
        "        divider = K.sum(y_pred, axis=1) + K.epsilon()\n",
        "        l = -K.log(divident / divider)\n",
        "    return l*1e4"
      ],
      "metadata": {
        "id": "ubB_bldXGnlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(chunk_size, context_samples=100, contrastive_samples=10, emd_size=512, gru_size=256):\n",
        "    \"\"\"\n",
        "\n",
        "    :param chunk_size:\n",
        "    :param context_samples:\n",
        "    :param contrastive_samples:\n",
        "    :param emd_size:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    K.set_learning_phase(1)\n",
        "\n",
        "    # Define encoder model\n",
        "    encoder_input = Input(shape=[chunk_size, 1])\n",
        "    encoder_model = Model(encoder_input, get_encoder(encoder_input, emb_size=emd_size), name='encoder')\n",
        "    encoder_model.summary()\n",
        "\n",
        "    # Define rest of the model\n",
        "    x_input = Input(shape=[context_samples, chunk_size, 1], name='context_data')\n",
        "    y_input = Input(shape=[contrastive_samples, chunk_size, 1], name='contrastive_data')\n",
        "\n",
        "    # Workaround context\n",
        "    x_encoded = TimeDistributed(encoder_model, name='Historical_embeddings')(x_input)\n",
        "    context = network_autoregressive(x_encoded, gru_size)\n",
        "    context = Lambda(lambda x: expand_dims(x, axis=-1), name='transpose_context')(context)\n",
        "\n",
        "    # Make predictions for the next predict_terms timesteps\n",
        "    z = TimeDistributed(encoder_model, name='Contrastive_embeddings')(y_input)\n",
        "    # Equation 3\n",
        "    z2 = Dense(units=gru_size, name='W', use_bias=False)(z)\n",
        "    z2 = Lambda(lambda x: K.permute_dimensions(x, (0, 2, 1)), name='transpose')(z2)\n",
        "    d = Lambda(lambda x: Dot(axes=1)(x), name='multiplication')([z2, context])\n",
        "\n",
        "    f = Lambda(lambda x: K.exp(x), name='exponent')(d)\n",
        "\n",
        "    # Model\n",
        "    cpc_model = Model(inputs=[x_input, y_input], outputs=f) #, y_labels\n",
        "    cpc_model.summary()\n",
        "    return cpc_model"
      ],
      "metadata": {
        "id": "FHefrS_RGuQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    tmr = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
        "    # params\n",
        "    K.set_learning_phase(1)\n",
        "    chunk_size = 4096\n",
        "    context_samples = 5\n",
        "    contrastive_samples = 1\n",
        "    emd_size = 512\n",
        "    batch_size = 8\n",
        "\n",
        "    params = {'model_name': 'cpc1'}\n",
        "    params.update({'checkpointer': {'verbose': 1,\n",
        "                                   'save_best_only': True,\n",
        "                                   'mode': 'min',\n",
        "                                    'monitor': 'loss'}})\n",
        "\n",
        "    model_params = {'chunk_size': chunk_size,\n",
        "                    'context_samples': context_samples,\n",
        "                    'contrastive_samples': contrastive_samples,\n",
        "                    'emd_size': emd_size}\n",
        "\n",
        "    #categories = ['Marimba_and_xylophone', 'Scissors', 'Gong', 'Printer', 'Keys_jangling', 'Zipper_(clothing)',\n",
        "    #              'Computer_keyboard', 'Finger_snapping']\n",
        "\n",
        "    categories = ()\n",
        "    gen_params = {'categories': categories,\n",
        "                  'data_pth': '/',\n",
        "                  'batch_size': batch_size,\n",
        "                  'shuffle': True,\n",
        "                  'seed': 42,\n",
        "                  'chunk_size': chunk_size,\n",
        "                  'context_samples': context_samples,\n",
        "                  'contrastive_samples': contrastive_samples}\n",
        "\n",
        "    output_folder = 'models'\n",
        "    tensorboard = TensorBoard(log_dir='./logs/' + 'cpc' + '_' + tmr,\n",
        "                              write_graph=True)\n",
        "    checkpointer = ModelCheckpoint(filepath=os.path.join(output_folder, params.get('model_name')+'.hdf5'),\n",
        "                                   **params['checkpointer'])\n",
        "\n",
        "    callbacks = [tensorboard, checkpointer]\n",
        "    model = get_model(**model_params)\n",
        "\n",
        "    data_gen = ContrastiveDataGenerator(**gen_params)\n",
        "\n",
        "    model.compile(loss=loss_fn, optimizer=Adam(lr=1e-5))\n",
        "    model.fit_generator(generator=data_gen, epochs=10, callbacks=callbacks)"
      ],
      "metadata": {
        "id": "uOnunLBYGzMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kh5dIZlKG3Z_",
        "outputId": "f665a548-949a-47cb-f01a-f8c895e0b80b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:414: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 4096, 1)]         0         \n",
            "                                                                 \n",
            " conv1d_20 (Conv1D)          (None, 819, 10)           40        \n",
            "                                                                 \n",
            " leaky_re_lu_20 (LeakyReLU)  (None, 819, 10)           0         \n",
            "                                                                 \n",
            " batch_normalization_20 (Bat  (None, 819, 10)          40        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv1d_21 (Conv1D)          (None, 205, 8)            248       \n",
            "                                                                 \n",
            " leaky_re_lu_21 (LeakyReLU)  (None, 205, 8)            0         \n",
            "                                                                 \n",
            " batch_normalization_21 (Bat  (None, 205, 8)           32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv1d_22 (Conv1D)          (None, 102, 4)            100       \n",
            "                                                                 \n",
            " leaky_re_lu_22 (LeakyReLU)  (None, 102, 4)            0         \n",
            "                                                                 \n",
            " batch_normalization_22 (Bat  (None, 102, 4)           16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv1d_23 (Conv1D)          (None, 50, 4)             52        \n",
            "                                                                 \n",
            " leaky_re_lu_23 (LeakyReLU)  (None, 50, 4)             0         \n",
            "                                                                 \n",
            " batch_normalization_23 (Bat  (None, 50, 4)            16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv1d_24 (Conv1D)          (None, 24, 4)             52        \n",
            "                                                                 \n",
            " leaky_re_lu_24 (LeakyReLU)  (None, 24, 4)             0         \n",
            "                                                                 \n",
            " batch_normalization_24 (Bat  (None, 24, 4)            16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 96)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 512)               49664     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 50,276\n",
            "Trainable params: 50,216\n",
            "Non-trainable params: 60\n",
            "_________________________________________________________________\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " contrastive_data (InputLayer)  [(None, 1, 4096, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " context_data (InputLayer)      [(None, 5, 4096, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " Contrastive_embeddings (TimeDi  (None, 1, 512)      50276       ['contrastive_data[0][0]']       \n",
            " stributed)                                                                                       \n",
            "                                                                                                  \n",
            " Historical_embeddings (TimeDis  (None, 5, 512)      50276       ['context_data[0][0]']           \n",
            " tributed)                                                                                        \n",
            "                                                                                                  \n",
            " W (Dense)                      (None, 1, 256)       131072      ['Contrastive_embeddings[0][0]'] \n",
            "                                                                                                  \n",
            " autoregressive_context (GRU)   (None, 256)          591360      ['Historical_embeddings[0][0]']  \n",
            "                                                                                                  \n",
            " transpose (Lambda)             (None, 256, 1)       0           ['W[0][0]']                      \n",
            "                                                                                                  \n",
            " transpose_context (Lambda)     (None, 256, 1)       0           ['autoregressive_context[0][0]'] \n",
            "                                                                                                  \n",
            " multiplication (Lambda)        (None, 1, 1)         0           ['transpose[0][0]',              \n",
            "                                                                  'transpose_context[0][0]']      \n",
            "                                                                                                  \n",
            " exponent (Lambda)              (None, 1, 1)         0           ['multiplication[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 772,708\n",
            "Trainable params: 772,648\n",
            "Non-trainable params: 60\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0012\n",
            "Epoch 00001: loss improved from inf to 0.00119, saving model to models/cpc1.hdf5\n",
            "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
            "7/7 [==============================] - 7s 109ms/step - loss: 0.0012\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0011\n",
            "Epoch 00002: loss improved from 0.00119 to 0.00109, saving model to models/cpc1.hdf5\n",
            "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
            "7/7 [==============================] - 1s 116ms/step - loss: 0.0011\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0227\n",
            "Epoch 00003: loss did not improve from 0.00109\n",
            "7/7 [==============================] - 1s 81ms/step - loss: 0.0227\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - ETA: 0s - loss: 7.7699e-04\n",
            "Epoch 00004: loss improved from 0.00109 to 0.00078, saving model to models/cpc1.hdf5\n",
            "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
            "7/7 [==============================] - 1s 109ms/step - loss: 7.7699e-04\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - ETA: 0s - loss: 6.7055e-04\n",
            "Epoch 00005: loss improved from 0.00078 to 0.00067, saving model to models/cpc1.hdf5\n",
            "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
            "7/7 [==============================] - 1s 110ms/step - loss: 6.7055e-04\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - ETA: 0s - loss: 6.0669e-04\n",
            "Epoch 00006: loss improved from 0.00067 to 0.00061, saving model to models/cpc1.hdf5\n",
            "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
            "7/7 [==============================] - 1s 110ms/step - loss: 6.0669e-04\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - ETA: 0s - loss: 9.1536e-04\n",
            "Epoch 00007: loss did not improve from 0.00061\n",
            "7/7 [==============================] - 1s 82ms/step - loss: 9.1536e-04\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - ETA: 0s - loss: 7.9828e-04\n",
            "Epoch 00008: loss did not improve from 0.00061\n",
            "7/7 [==============================] - 1s 81ms/step - loss: 7.9828e-04\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - ETA: 0s - loss: 9.6858e-04\n",
            "Epoch 00009: loss did not improve from 0.00061\n",
            "7/7 [==============================] - 1s 80ms/step - loss: 9.6858e-04\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0098\n",
            "Epoch 00010: loss did not improve from 0.00061\n",
            "7/7 [==============================] - 1s 81ms/step - loss: 0.0098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aMVdjvxWHhGS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}