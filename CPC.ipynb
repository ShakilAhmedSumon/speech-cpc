{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CPC.ipynb",
      "provenance": [],
      "mount_file_id": "1NCCekMjkVai1aJuqzcktdtbIkPjgbKVm",
      "authorship_tag": "ABX9TyNATtOQB07+2Hl7WW0NOY9c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShakilAhmedSumon/speech-cpc/blob/main/CPC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fIN_iaxM8Z5_"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import Sequence\n",
        "from random import shuffle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import os\n",
        "import logging\n",
        "import warnings\n",
        "from random import shuffle\n",
        "import numpy as np\n",
        "\n",
        "from keras.layers import Conv1D, BatchNormalization, LeakyReLU, Flatten, Dense, GRU, TimeDistributed, Input, Lambda\n",
        "from keras.layers import Dot, Lambda\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from keras import backend as K\n",
        "from keras.backend import expand_dims\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_logging(fname, level=logging.DEBUG):\n",
        "    \"\"\"\n",
        "    Create logger instance\n",
        "    :param fname: name of log file\n",
        "    :param level: log level\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    formatter = logging.Formatter('[%(levelname)s]%(asctime)s:%(name)s:%(message)s')\n",
        "    logger = logging.getLogger()\n",
        "    logger.setLevel(level)\n",
        "\n",
        "    # File Handler\n",
        "    fh = logging.FileHandler(fname)\n",
        "    fh.setLevel(level)\n",
        "    fh.setFormatter(formatter)\n",
        "    logger.addHandler(fh)\n",
        "\n",
        "    # Stream Handler\n",
        "    ch = logging.StreamHandler()\n",
        "    ch.setLevel(logging.WARNING)\n",
        "    ch.setFormatter(formatter)\n",
        "    logger.addHandler(ch)"
      ],
      "metadata": {
        "id": "5_e6gOkf-ZqF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ContrastiveDataGenerator(Sequence):\n",
        "\n",
        "    def __init__(self, data_pth='../data', batch_size=10, shuffle=True, seed=42, categories=list(), normalize=True,\n",
        "                 fs=16000, chunk_size=4096, context_samples=5, contrastive_samples=1):\n",
        "        \"\"\"\n",
        "        Constructor\n",
        "\n",
        "        :param data_file: path to data file\n",
        "        :param meta_file:  path to meta file\n",
        "        :param batch_size: batch size\n",
        "        :param measurement_ids: list of measurement ids. Dedicated for CV\n",
        "        :param shuffle:\n",
        "        :param seed: random seed\n",
        "        :param test_mode: return samples and signal_ids\n",
        "        :param normalize: to normalize the data\n",
        "        \"\"\"\n",
        "        self.it = 0\n",
        "        self.shuffle = shuffle\n",
        "        self.data_pth = data_pth\n",
        "        self.normalize = normalize\n",
        "        self.fs = fs\n",
        "        self.batch_size = batch_size\n",
        "        self.seed = seed\n",
        "        self.context_samples = int(context_samples)\n",
        "        self.contrastive_samples = int(contrastive_samples)\n",
        "        self.chunk_size = int(chunk_size)\n",
        "\n",
        "        # Extract list of files from csv\n",
        "        # file_list = pd.read_csv(os.path.join(data_pth, 'train_curated.csv'))\n",
        "        file_list = os.listdir()\n",
        "        if len(categories) == 0:\n",
        "            # self.file_list = file_list.fname.tolist()\n",
        "            self.file_list = file_list\n",
        "        else:\n",
        "            self.file_list = file_list.query('labels in @categories').fname.tolist()\n",
        "        self.list_sz = len(self.file_list)\n",
        "        self.max_it = int(np.ceil(self.list_sz / self.batch_size))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.max_it\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"\n",
        "        Performs at the end of each epoch\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        l = self.file_list\n",
        "        shuffle(l)\n",
        "        self.file_list = l\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        \"\"\"\n",
        "        Return one batch\n",
        "        :param item:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        return self.__data_generation(item)\n",
        "\n",
        "    def __data_generation(self, it):\n",
        "        \"\"\"\n",
        "        Data generator\n",
        "        :param it:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        pos = np.minimum(it * self.batch_size, self.list_sz)\n",
        "        frames = (self.contrastive_samples+self.context_samples)*self.chunk_size\n",
        "\n",
        "        i = 0\n",
        "        context_batch = np.zeros([self.batch_size, self.context_samples, self.chunk_size])\n",
        "        contrastive_batch = np.zeros([self.batch_size, self.contrastive_samples, self.chunk_size])\n",
        "\n",
        "        while i < self.batch_size:\n",
        "            fname = self.file_list[pos]\n",
        "            pos = (pos+1) % self.list_sz\n",
        "            signal, sr = librosa.load(os.path.join(self.data_pth, fname), sr=self.fs)\n",
        "            if signal.shape[0]-frames < 0:\n",
        "                logging.getLogger(__name__).info(' File {:s} is too short'.format(fname))\n",
        "            else:\n",
        "                random_shift = np.random.randint(signal.shape[0]-frames)\n",
        "                batch = signal[random_shift:(frames + random_shift)].reshape((-1, self.chunk_size), order='C')\n",
        "                context_batch[i, :, :] = batch[:self.context_samples, :]\n",
        "                contrastive_batch[i, :, :] = batch[self.context_samples:self.context_samples+self.contrastive_samples, :]\n",
        "                i +=1\n",
        "\n",
        "        # shuffle data\n",
        "        #idx = np.random.choice(range(self.batch_size), self.batch_size, replace=False)\n",
        "        #contrastive_batch = contrastive_batch[idx, :, :]\n",
        "        labels=np.zeros([self.batch_size, self.batch_size])\n",
        "        labels=np.identity(self.batch_size)\n",
        "        labels = labels[:, :, np.newaxis]\n",
        "        #labels[range(self.batch_size), idx] = 1\n",
        "        s = ([context_batch[:, :, :, np.newaxis], contrastive_batch[:, :, :, np.newaxis]], labels)\n",
        "        return s"
      ],
      "metadata": {
        "id": "98VH5wh__I2i"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDmOsZ4p_SoZ",
        "outputId": "d077682c-f3b1-402f-bb91-18366713dadb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JeByafK7EBo1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4GIYrGWEfxc",
        "outputId": "1d650db5-4dc0-426f-e609-c7e3b4bb9204"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "174-50561-0000.flac  174-50561-0017.flac  84-121123-0014.flac\n",
            "174-50561-0001.flac  174-50561-0018.flac  84-121123-0015.flac\n",
            "174-50561-0002.flac  174-50561-0019.flac  84-121123-0016.flac\n",
            "174-50561-0003.flac  84-121123-0000.flac  84-121123-0017.flac\n",
            "174-50561-0004.flac  84-121123-0001.flac  84-121123-0018.flac\n",
            "174-50561-0005.flac  84-121123-0002.flac  84-121123-0019.flac\n",
            "174-50561-0006.flac  84-121123-0003.flac  84-121123-0020.flac\n",
            "174-50561-0007.flac  84-121123-0004.flac  84-121123-0021.flac\n",
            "174-50561-0008.flac  84-121123-0005.flac  84-121123-0022.flac\n",
            "174-50561-0009.flac  84-121123-0006.flac  84-121123-0023.flac\n",
            "174-50561-0010.flac  84-121123-0007.flac  84-121123-0024.flac\n",
            "174-50561-0011.flac  84-121123-0008.flac  84-121123-0025.flac\n",
            "174-50561-0012.flac  84-121123-0009.flac  84-121123-0026.flac\n",
            "174-50561-0013.flac  84-121123-0010.flac  84-121123-0027.flac\n",
            "174-50561-0014.flac  84-121123-0011.flac  84-121123-0028.flac\n",
            "174-50561-0015.flac  84-121123-0012.flac\n",
            "174-50561-0016.flac  84-121123-0013.flac\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_list = os.listdir()"
      ],
      "metadata": {
        "id": "I8x0wLe1EndI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_encoder(x, emb_size):\n",
        "    \"\"\"\n",
        "    Create encoder\n",
        "    :param x:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    with tf.name_scope('Encoder'):\n",
        "        with tf.name_scope('embedding_level_1'):\n",
        "            x = Conv1D(filters=10, strides=5, kernel_size=3)(x)\n",
        "            x = LeakyReLU()(x)\n",
        "            x = BatchNormalization()(x)\n",
        "\n",
        "        with tf.name_scope('embedding_level_2'):\n",
        "            x = Conv1D(filters=8, strides=4, kernel_size=3)(x)\n",
        "            x = LeakyReLU()(x)\n",
        "            x = BatchNormalization()(x)\n",
        "\n",
        "        with tf.name_scope('embedding_level_2'):\n",
        "            x = Conv1D(filters=4, strides=2, kernel_size=3)(x)\n",
        "            x = LeakyReLU()(x)\n",
        "            x = BatchNormalization()(x)\n",
        "\n",
        "        with tf.name_scope('embedding_level_4'):\n",
        "            x = Conv1D(filters=4, strides=2, kernel_size=3)(x)\n",
        "            x = LeakyReLU()(x)\n",
        "            x = BatchNormalization()(x)\n",
        "\n",
        "        with tf.name_scope('embedding_level_5'):\n",
        "            x = Conv1D(filters=4, strides=2, kernel_size=3)(x)\n",
        "            x = LeakyReLU()(x)\n",
        "            x = BatchNormalization()(x)\n",
        "\n",
        "        with tf.name_scope('embedding_dense'):\n",
        "            x = Flatten()(x)\n",
        "            x = Dense(units=emb_size, activation='relu')(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "eN4C2AFQEvTc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def network_autoregressive(x, code_size):\n",
        "    \"\"\"\n",
        "    Define the network that integrates information along the sequence\n",
        "    :param x:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return GRU(units=code_size, return_sequences=False, name='autoregressive_context')(x)\n"
      ],
      "metadata": {
        "id": "PBLKRaiDGkle"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Contrastive loss function (eq. 4 from the original article)\n",
        "    # https://datascience.stackexchange.com/questions/25029/custom-loss-function-with-additional-parameter-in-keras\n",
        "    :param y_true: labels (0, 1), where 0 means the sample was drawn from noisy distribution; 1 means the sample was\n",
        "    drawn from the target distribution.\n",
        "    :param y_pred: density ratio (f value from the original article)\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    with tf.name_scope('custom_loss_function'):\n",
        "        divident = K.sum(K.dot(y_true, y_pred), axis=1)\n",
        "        divider = K.sum(y_pred, axis=1) + K.epsilon()\n",
        "        l = -K.log(divident / divider)\n",
        "    return l*1e4"
      ],
      "metadata": {
        "id": "ubB_bldXGnlS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(chunk_size, context_samples=100, contrastive_samples=10, emd_size=512, gru_size=256):\n",
        "    \"\"\"\n",
        "\n",
        "    :param chunk_size:\n",
        "    :param context_samples:\n",
        "    :param contrastive_samples:\n",
        "    :param emd_size:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    K.set_learning_phase(1)\n",
        "\n",
        "    # Define encoder model\n",
        "    encoder_input = Input(shape=[chunk_size, 1])\n",
        "    encoder_model = Model(encoder_input, get_encoder(encoder_input, emb_size=emd_size), name='encoder')\n",
        "    encoder_model.summary()\n",
        "\n",
        "    # Define rest of the model\n",
        "    x_input = Input(shape=[context_samples, chunk_size, 1], name='context_data')\n",
        "    y_input = Input(shape=[contrastive_samples, chunk_size, 1], name='contrastive_data')\n",
        "\n",
        "    # Workaround context\n",
        "    x_encoded = TimeDistributed(encoder_model, name='Historical_embeddings')(x_input)\n",
        "    context = network_autoregressive(x_encoded, gru_size)\n",
        "    context = Lambda(lambda x: expand_dims(x, axis=-1), name='transpose_context')(context)\n",
        "\n",
        "    # Make predictions for the next predict_terms timesteps\n",
        "    z = TimeDistributed(encoder_model, name='Contrastive_embeddings')(y_input)\n",
        "    # Equation 3\n",
        "    z2 = Dense(units=gru_size, name='W', use_bias=False)(z)\n",
        "    z2 = Lambda(lambda x: K.permute_dimensions(x, (0, 2, 1)), name='transpose')(z2)\n",
        "    d = Lambda(lambda x: Dot(axes=1)(x), name='multiplication')([z2, context])\n",
        "\n",
        "    f = Lambda(lambda x: K.exp(x), name='exponent')(d)\n",
        "\n",
        "    # Model\n",
        "    cpc_model = Model(inputs=[x_input, y_input], outputs=f) #, y_labels\n",
        "    cpc_model.summary()\n",
        "    return cpc_model"
      ],
      "metadata": {
        "id": "FHefrS_RGuQ7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    tmr = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
        "    # params\n",
        "    K.set_learning_phase(1)\n",
        "    chunk_size = 4096\n",
        "    context_samples = 5\n",
        "    contrastive_samples = 1\n",
        "    emd_size = 512\n",
        "    batch_size = 8\n",
        "\n",
        "    params = {'model_name': 'cpc1'}\n",
        "    params.update({'checkpointer': {'verbose': 1,\n",
        "                                   'save_best_only': True,\n",
        "                                   'mode': 'min',\n",
        "                                    'monitor': 'loss'}})\n",
        "\n",
        "    model_params = {'chunk_size': chunk_size,\n",
        "                    'context_samples': context_samples,\n",
        "                    'contrastive_samples': contrastive_samples,\n",
        "                    'emd_size': emd_size}\n",
        "\n",
        "    #categories = ['Marimba_and_xylophone', 'Scissors', 'Gong', 'Printer', 'Keys_jangling', 'Zipper_(clothing)',\n",
        "    #              'Computer_keyboard', 'Finger_snapping']\n",
        "\n",
        "    categories = ()\n",
        "    gen_params = {'categories': categories,\n",
        "                  'data_pth': '/',\n",
        "                  'batch_size': batch_size,\n",
        "                  'shuffle': True,\n",
        "                  'seed': 42,\n",
        "                  'chunk_size': chunk_size,\n",
        "                  'context_samples': context_samples,\n",
        "                  'contrastive_samples': contrastive_samples}\n",
        "\n",
        "    output_folder = 'models'\n",
        "    tensorboard = TensorBoard(log_dir='./logs/' + 'cpc' + '_' + tmr,\n",
        "                              write_graph=True)\n",
        "    checkpointer = ModelCheckpoint(filepath=os.path.join(output_folder, params.get('model_name')+'.hdf5'),\n",
        "                                   **params['checkpointer'])\n",
        "\n",
        "    callbacks = [tensorboard, checkpointer]\n",
        "    model = get_model(**model_params)\n",
        "\n",
        "    data_gen = ContrastiveDataGenerator(**gen_params)\n",
        "\n",
        "    model.compile(loss=loss_fn, optimizer=Adam(lr=1e-5))\n",
        "    model.fit_generator(generator=data_gen, epochs=10, callbacks=callbacks)"
      ],
      "metadata": {
        "id": "uOnunLBYGzMC"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Kh5dIZlKG3Z_",
        "outputId": "d3ba59d9-9a10-416e-b645-7dfb4bb15c4f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:414: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 4096, 1)]         0         \n",
            "                                                                 \n",
            " conv1d_15 (Conv1D)          (None, 819, 10)           40        \n",
            "                                                                 \n",
            " leaky_re_lu_15 (LeakyReLU)  (None, 819, 10)           0         \n",
            "                                                                 \n",
            " batch_normalization_15 (Bat  (None, 819, 10)          40        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv1d_16 (Conv1D)          (None, 205, 8)            248       \n",
            "                                                                 \n",
            " leaky_re_lu_16 (LeakyReLU)  (None, 205, 8)            0         \n",
            "                                                                 \n",
            " batch_normalization_16 (Bat  (None, 205, 8)           32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv1d_17 (Conv1D)          (None, 102, 4)            100       \n",
            "                                                                 \n",
            " leaky_re_lu_17 (LeakyReLU)  (None, 102, 4)            0         \n",
            "                                                                 \n",
            " batch_normalization_17 (Bat  (None, 102, 4)           16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv1d_18 (Conv1D)          (None, 50, 4)             52        \n",
            "                                                                 \n",
            " leaky_re_lu_18 (LeakyReLU)  (None, 50, 4)             0         \n",
            "                                                                 \n",
            " batch_normalization_18 (Bat  (None, 50, 4)            16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv1d_19 (Conv1D)          (None, 24, 4)             52        \n",
            "                                                                 \n",
            " leaky_re_lu_19 (LeakyReLU)  (None, 24, 4)             0         \n",
            "                                                                 \n",
            " batch_normalization_19 (Bat  (None, 24, 4)            16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 96)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 512)               49664     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 50,276\n",
            "Trainable params: 50,216\n",
            "Non-trainable params: 60\n",
            "_________________________________________________________________\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " contrastive_data (InputLayer)  [(None, 1, 4096, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " context_data (InputLayer)      [(None, 5, 4096, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " Contrastive_embeddings (TimeDi  (None, 1, 512)      50276       ['contrastive_data[0][0]']       \n",
            " stributed)                                                                                       \n",
            "                                                                                                  \n",
            " Historical_embeddings (TimeDis  (None, 5, 512)      50276       ['context_data[0][0]']           \n",
            " tributed)                                                                                        \n",
            "                                                                                                  \n",
            " W (Dense)                      (None, 1, 256)       131072      ['Contrastive_embeddings[0][0]'] \n",
            "                                                                                                  \n",
            " autoregressive_context (GRU)   (None, 256)          591360      ['Historical_embeddings[0][0]']  \n",
            "                                                                                                  \n",
            " transpose (Lambda)             (None, 256, 1)       0           ['W[0][0]']                      \n",
            "                                                                                                  \n",
            " transpose_context (Lambda)     (None, 256, 1)       0           ['autoregressive_context[0][0]'] \n",
            "                                                                                                  \n",
            " multiplication (Lambda)        (None, 1, 1)         0           ['transpose[0][0]',              \n",
            "                                                                  'transpose_context[0][0]']      \n",
            "                                                                                                  \n",
            " exponent (Lambda)              (None, 1, 1)         0           ['multiplication[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 772,708\n",
            "Trainable params: 772,648\n",
            "Non-trainable params: 60\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-2d3d3b41a125>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2028\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2029\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2030\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2032\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_not_generate_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-73c7d019677b>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__data_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__data_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-73c7d019677b>\u001b[0m in \u001b[0;36m__data_generation\u001b[0;34m(self, it)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_sz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_pth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mframes\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' File {:s} is too short'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPurePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PySoundFile failed. Trying audioread instead.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__audioread_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36m__audioread_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0maudioread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mn_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/audioread/__init__.py\u001b[0m in \u001b[0;36maudio_open\u001b[0;34m(path, backends)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mBackendClass\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbackends\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mBackendClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/audioread/rawread.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \"\"\"\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/84-121123-0000.flac'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aMVdjvxWHhGS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}